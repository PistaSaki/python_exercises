{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a custom regression model without Keras\n",
    "\n",
    "Suggested reading:\n",
    "* https://www.tensorflow.org/tutorials/customization/autodiff\n",
    "* https://www.tensorflow.org/tutorials/customization/custom_training\n",
    "\n",
    "\n",
    "## Our Model\n",
    "Denote by $X$ the maximal daily temperature (in Celsius) at a given day and by $Y$ a number of ice-creams sold on that day. We want to model dependency of $Y$ on $X$. Consider the model \n",
    "$$ Y \\sim \\mathrm{Poisson}\\big[rate=\\mathrm{softplus}(a X + b)\\big]$$\n",
    "where $a, b\\in \\mathbb R$ are unknown constants. \n",
    "\n",
    "## Generate some artificial data\n",
    "We assume $a = 0.1$; $b=4$ are the real values of the constants we want to find and we have 1000 observations. Thus our data consist of vectors `X`, `Y` of length 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow_probability import distributions as tfd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "\n",
    "X = tfd.Normal(0, 10).sample(1000)\n",
    "print(\"First few elements of X:\", X[:5].numpy())\n",
    "\n",
    "rate = tf.nn.softplus(0.1 * X + 4)\n",
    "\n",
    "Y = tfd.Poisson(rate).sample()\n",
    "print(\"First few elements of Y:\", Y[:5].numpy())\n",
    "\n",
    "# plotting\n",
    "plt.figure(figsize=[9, 3])\n",
    "ax = plt.gca()\n",
    "ax.scatter(X, Y)\n",
    "ax.set_xlabel(\"X\")\n",
    "ax.set_ylabel(\"Y\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "We will find the parameters by maximizing the likelihood.\n",
    "\n",
    "### Exercise 1\n",
    "Write a function calculating the `loss` that is the negative log-likelihood of the parameters for given data. Make sure you use only `tensorflow` and `tensorflow-probability`, not `numpy` or `scipy`.\n",
    "\n",
    "Hint: I would use the following functions: `tf.nn.softplus`, `tf.reduce_sum` and `log_prob` method of `tfd.Poisson`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(a, b, X, Y):\n",
    "    \"\"\" Return the negative log-likelihood of the params `a, b`. \n",
    "    \n",
    "    Args:\n",
    "        a, b: scalars; parameters of our model\n",
    "        X: vector of measured daily temperatures\n",
    "        Y: observed numbers of sold ice-creams on the corresponding days\n",
    "        \n",
    "    Returns:\n",
    "        scalar `- log( P[Y | a, b, X] )`\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a little test\n",
    "l = get_loss(a=1., b=2., X=tf.constant([-10., 0., 10.]), Y=[1, 2, 3])\n",
    "assert np.allclose(l, 15.648288)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-up our training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The current estimate of each parameter will be stored in a `tf.Variable`\n",
    "# We initialize these variables with some arbitrary numbers.\n",
    "\n",
    "a = tf.Variable(1, name=\"a\", dtype=K.floatx())\n",
    "b = tf.Variable(0, name=\"b\", dtype=K.floatx())\n",
    "weights = [a, b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the `optimizer` to be used for minimization of the loss\n",
    "optimizer = tf.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "In the following cell complete the inside of the optimization loop -- evaluate the current `loss` and use the `tf.GradientTape` to calculate the gradient with respect to the weights. Assign the gradient to `grads`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(10000):\n",
    "    # calculate current loss and use `tf.GradientTape` to calculate loss gradients with respect to the `weights`\n",
    "    ## Your code here:\n",
    "    \n",
    "    \n",
    "    grads = tape.gradient(loss, weights)\n",
    "    \n",
    "    # using the `optimizer` we can use the calculated gradients to update the values of `weights`\n",
    "    optimizer.apply_gradients(zip(grads, weights))\n",
    "\n",
    "print(loss.numpy(), a.numpy(), b.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speed things up using `tf.function`\n",
    "Try so read the \"basics\" part of the following tutorial:\n",
    "https://www.tensorflow.org/tutorials/customization/performance\n",
    "\n",
    "Using `tf.function` decorator often speeds the calculations a lot! (And sometimes not.) Unfortunately not all the code that works well in the eager-mode (un-decorated) compiles well to graph-mode (decorated). If you will use it, be prapared for some fight. \n",
    "\n",
    "### Exercise 3\n",
    "Copy your `quadratic_fun` from the previous exercise. \n",
    "Use the `%%timeit` magic to measure the time we need to run it.\n",
    "Then use `tf.function` decorator and measure the time again. Here the difference in speed will not be large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic_fun(a, b, c, x):\n",
    "    # Your code comes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "quadratic_fun(a=tf.zeros([n, n]), b=tf.zeros(n), c=0, x=tf.ones([1000, n]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "copy the content of the optimization for-loop in the first exercise into one function without arguments \n",
    "```python\n",
    "def train_step():\n",
    "    # your code here\n",
    "```\n",
    "Then use `%timeit` magic to see the difference in speed of the decorated and un-decorated versions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technical Remark\n",
    "Feel free to skip this.\n",
    "\n",
    "Decorating by `tf.function` does not do much. The compilation to the computation-graph happens only when the decorated function is called for the first time. The `tensorflow` then knows what are the shapes of the incomming tensors and creates a graph whose inputs have the corresponding shapes. This compilation is quite time-consuming, but the good thing is that happens only once. \n",
    "\n",
    "Unless you call the decorated function with arguments of different shapes! In that case it happens again. Fortunately you can specify the input-shapes in advance, including not-completely defined shapes. The tensorflow then creates one graph whose imputs have partially defined shapes and uses this graph for all subsequent evaluations of the decorated function. To do this, you must specify the `input_signature` parameter of the `tf.function`.\n",
    "\n",
    "As an example consider the case when we want to use our `quadratic_fun` only in 5-dimensional space and with 1-dimensional batches of vectors `x`. However the size of the batch is unknown in advance and can change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "input_signature=[tf.TensorSpec([n, n], K.floatx()), \n",
    "                 tf.TensorSpec([n], K.floatx()), \n",
    "                 tf.TensorSpec([], K.floatx()), \n",
    "                 tf.TensorSpec([None, n], K.floatx())  # `x` has unknown batch-size\n",
    "                ]\n",
    "\n",
    "@tf.function(input_signature=input_signature)\n",
    "def quadratic_fun__decorated(a, b, c, x):\n",
    "    # Using Honza's calculation:\n",
    "    return tf.reduce_sum((tf.linalg.matvec(a, x) + b) * x, axis = [-1]) + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quadratic_fun__decorated(tf.zeros([5, 5]), tf.zeros([5]), 0., tf.zeros([2, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quadratic_fun__decorated(tf.zeros([5, 5]), tf.zeros([5]), 0., tf.zeros([3, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The following evaluation now fails since the inputs are not consistent with the `input_signature`\n",
    "quadratic_fun__decorated(tf.zeros([6, 6]), tf.zeros([6]), 0., tf.zeros([3, 6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional reading - Datasets\n",
    "Read first part \"Basic Mechanics\" of this tutorial:\n",
    "https://www.tensorflow.org/guide/data#basic_mechanics\n",
    "\n",
    "### Optional Exercise 5\n",
    "Let's return to our ice-cream regression.\n",
    "Imagine that we can't evaluate our model (because it is too complicated and we have too much data) at once and we want to use the stochastic-descent on mini-batches created from the data. \n",
    "* Use `tf.data.Dataset.from_tensor_slices` to create a dataset from our data tensors `X`, `Y`\n",
    "* use `shuffle` and `batch` methods of the dataset to create a shuffled and batched dataset, say with batches of length 100.\n",
    "* Write a new training loop where in each epoch you iterate through all the batches in the batched data-set and apply the gradient descent (or a fancier optimizer optimizer like adam) to each batch separately."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
